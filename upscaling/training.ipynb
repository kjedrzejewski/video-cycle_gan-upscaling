{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Network import Generator, Discriminator\n",
    "import Utils_model, Utils\n",
    "from Utils_model import VGG_LOSS\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# Better to use downscale factor as 4\n",
    "downscale_factor = 3.75\n",
    "# Remember to change image shape if you are having different size of images\n",
    "image_shape = (1920,1080,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, shape, generator, optimizer, vgg_loss):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=shape)\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=[x,gan_output])\n",
    "    gan.compile(loss=[vgg_loss, \"binary_crossentropy\"],\n",
    "                loss_weights=[1., 1e-3],\n",
    "                optimizer=optimizer)\n",
    "\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 1\n",
    "input_dir = '../images/photo_fullhd'\n",
    "output_dir = 'output'\n",
    "model_save_dir = 'trained_model'\n",
    "number_of_images = 320\n",
    "train_test_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files::   9%|▉         | 318/3518 [00:13<02:28, 21.61it/s]\n",
      "Converting to low-res:   0%|          | 0/256 [00:00<?, ?it/s]\u001b[A\n",
      "Converting to low-res:   2%|▏         | 6/256 [00:00<00:04, 57.31it/s]\u001b[A\n",
      "Converting to low-res:   5%|▌         | 13/256 [00:00<00:04, 59.53it/s]\u001b[A\n",
      "Converting to low-res:   8%|▊         | 20/256 [00:00<00:03, 61.41it/s]\u001b[A\n",
      "Converting to low-res:  11%|█         | 27/256 [00:00<00:03, 62.14it/s]\u001b[A\n",
      "Converting to low-res:  13%|█▎        | 34/256 [00:00<00:03, 62.55it/s]\u001b[A\n",
      "Converting to low-res:  16%|█▌        | 41/256 [00:00<00:03, 63.21it/s]\u001b[A\n",
      "Converting to low-res:  19%|█▉        | 48/256 [00:00<00:03, 63.70it/s]\u001b[A\n",
      "Converting to low-res:  21%|██▏       | 55/256 [00:00<00:03, 63.64it/s]\u001b[A\n",
      "Converting to low-res:  24%|██▍       | 62/256 [00:00<00:03, 63.42it/s]\u001b[A\n",
      "Converting to low-res:  27%|██▋       | 69/256 [00:01<00:02, 63.68it/s]\u001b[A\n",
      "Converting to low-res:  30%|██▉       | 76/256 [00:01<00:02, 63.62it/s]\u001b[A\n",
      "Converting to low-res:  32%|███▏      | 83/256 [00:01<00:02, 64.38it/s]\u001b[A\n",
      "Converting to low-res:  35%|███▌      | 90/256 [00:01<00:02, 64.32it/s]\u001b[A\n",
      "Converting to low-res:  38%|███▊      | 97/256 [00:01<00:02, 64.37it/s]\u001b[A\n",
      "Converting to low-res:  41%|████      | 104/256 [00:01<00:02, 64.55it/s]\u001b[A\n",
      "Converting to low-res:  43%|████▎     | 111/256 [00:01<00:02, 64.86it/s]\u001b[A\n",
      "Converting to low-res:  46%|████▌     | 118/256 [00:01<00:02, 64.83it/s]\u001b[A\n",
      "Converting to low-res:  49%|████▉     | 125/256 [00:01<00:02, 64.75it/s]\u001b[A\n",
      "Converting to low-res:  52%|█████▏    | 132/256 [00:02<00:01, 64.71it/s]\u001b[A\n",
      "Converting to low-res:  54%|█████▍    | 139/256 [00:02<00:01, 64.15it/s]\u001b[A\n",
      "Converting to low-res:  57%|█████▋    | 146/256 [00:02<00:01, 64.54it/s]\u001b[A\n",
      "Converting to low-res:  60%|█████▉    | 153/256 [00:02<00:01, 64.44it/s]\u001b[A\n",
      "Converting to low-res:  62%|██████▎   | 160/256 [00:02<00:01, 64.10it/s]\u001b[A\n",
      "Converting to low-res:  65%|██████▌   | 167/256 [00:02<00:01, 64.14it/s]\u001b[A\n",
      "Converting to low-res:  68%|██████▊   | 174/256 [00:02<00:01, 64.14it/s]\u001b[A\n",
      "Converting to low-res:  71%|███████   | 181/256 [00:02<00:01, 63.21it/s]\u001b[A\n",
      "Converting to low-res:  73%|███████▎  | 188/256 [00:02<00:01, 63.53it/s]\u001b[A\n",
      "Converting to low-res:  76%|███████▌  | 195/256 [00:03<00:00, 63.42it/s]\u001b[A\n",
      "Converting to low-res:  79%|███████▉  | 202/256 [00:03<00:00, 63.85it/s]\u001b[A\n",
      "Converting to low-res:  82%|████████▏ | 209/256 [00:03<00:00, 63.85it/s]\u001b[A\n",
      "Converting to low-res:  84%|████████▍ | 216/256 [00:03<00:00, 63.52it/s]\u001b[A\n",
      "Converting to low-res:  87%|████████▋ | 223/256 [00:03<00:00, 63.60it/s]\u001b[A\n",
      "Converting to low-res:  90%|████████▉ | 230/256 [00:03<00:00, 63.90it/s]\u001b[A\n",
      "Converting to low-res:  93%|█████████▎| 237/256 [00:03<00:00, 64.20it/s]\u001b[A\n",
      "Converting to low-res:  95%|█████████▌| 244/256 [00:03<00:00, 64.46it/s]\u001b[A\n",
      "Converting to low-res: 100%|██████████| 256/256 [00:04<00:00, 63.99it/s]\u001b[A\n",
      "\n",
      "Converting to low-res:   0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Converting to low-res:  11%|█         | 7/64 [00:00<00:00, 61.72it/s]\u001b[A\n",
      "Converting to low-res:  22%|██▏       | 14/64 [00:00<00:00, 62.94it/s]\u001b[A\n",
      "Converting to low-res:  33%|███▎      | 21/64 [00:00<00:00, 63.72it/s]\u001b[A\n",
      "Converting to low-res:  44%|████▍     | 28/64 [00:00<00:00, 63.91it/s]\u001b[A\n",
      "Converting to low-res:  55%|█████▍    | 35/64 [00:00<00:00, 64.03it/s]\u001b[A\n",
      "Converting to low-res:  66%|██████▌   | 42/64 [00:00<00:00, 64.61it/s]\u001b[A\n",
      "Converting to low-res:  77%|███████▋  | 49/64 [00:00<00:00, 65.03it/s]\u001b[A\n",
      "Converting to low-res:  88%|████████▊ | 56/64 [00:00<00:00, 64.73it/s]\u001b[A\n",
      "Converting to low-res: 100%|██████████| 64/64 [00:00<00:00, 64.55it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "x_train_lr, x_train_hr, x_test_lr, x_test_hr = Utils.load_training_data(input_dir, '.jpg', number_of_images, train_test_ratio, downscale_factor) \n",
    "loss = VGG_LOSS(image_shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = int(x_train_hr.shape[0] / batch_size)\n",
    "shape = (int(image_shape[0]//downscale_factor), int(image_shape[1]//downscale_factor), image_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0915 20:33:53.562437 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0915 20:33:53.591164 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0915 20:33:53.593585 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0915 20:33:53.630385 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0915 20:33:53.630930 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0915 20:33:53.712870 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0915 20:33:56.163499 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0915 20:33:56.963392 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0915 20:33:56.985714 4569007552 deprecation_wrapper.py:119] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Loading files::   9%|▉         | 318/3518 [00:30<02:28, 21.61it/s]W0915 20:35:17.283248 4569007552 deprecation.py:323] From /Users/kjedrzejewski/miniconda3/envs/gan_upscaling/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(shape).generator()\n",
    "discriminator = Discriminator(image_shape).discriminator()\n",
    "\n",
    "optimizer = Utils_model.get_optimizer()\n",
    "generator.compile(loss=loss.vgg_loss, optimizer=optimizer)\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "    \n",
    "gan = get_gan_network(discriminator, shape, generator, optimizer, loss.vgg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_file = open(model_save_dir + '/losses.txt' , 'w+')\n",
    "loss_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/256 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, epochs+1):\n",
    "    print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "    for _ in tqdm(range(batch_count)):\n",
    "        \n",
    "        rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
    "        \n",
    "        image_batch_hr = x_train_hr[rand_nums]\n",
    "        image_batch_lr = x_train_lr[rand_nums]\n",
    "        #generated_images_sr = generator.predict(image_batch_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a9099e5a7b416889126551ae0f6da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, epochs+1):\n",
    "    print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "    for _ in tqdm(range(batch_count)):\n",
    "\n",
    "        rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
    "\n",
    "        image_batch_hr = x_train_hr[rand_nums]\n",
    "        image_batch_lr = x_train_lr[rand_nums]\n",
    "        generated_images_sr = generator.predict(image_batch_lr)\n",
    "\n",
    "        real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "        fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
    "\n",
    "        discriminator.trainable = True\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(image_batch_hr, real_data_Y)\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_images_sr, fake_data_Y)\n",
    "        discriminator_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "        rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
    "        image_batch_hr = x_train_hr[rand_nums]\n",
    "        image_batch_lr = x_train_lr[rand_nums]\n",
    "\n",
    "        gan_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "        discriminator.trainable = False\n",
    "        gan_loss = gan.train_on_batch(image_batch_lr, [image_batch_hr,gan_Y])\n",
    "\n",
    "\n",
    "    print(\"discriminator_loss : %f\" % discriminator_loss)\n",
    "    print(\"gan_loss :\", gan_loss)\n",
    "    gan_loss = str(gan_loss)\n",
    "\n",
    "    loss_file = open(model_save_dir + 'losses.txt' , 'a')\n",
    "    loss_file.write('epoch%d : gan_loss = %s ; discriminator_loss = %f\\n' %(e, gan_loss, discriminator_loss) )\n",
    "    loss_file.close()\n",
    "\n",
    "    if e == 1 or e % 5 == 0:\n",
    "        Utils.plot_generated_images(output_dir, e, generator, x_test_hr, x_test_lr)\n",
    "    if e % 500 == 0:\n",
    "        generator.save(model_save_dir + 'gen_model%d.h5' % e)\n",
    "        discriminator.save(model_save_dir + 'dis_model%d.h5' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
