{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from skimage import data, io, filters\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy.random import randint\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from upscaler.data import load_images_from_dir, crop_images, select_random_rows, convert_image_series_to_array, downscale_images, crop_images_cgc\n",
    "from upscaler.data import select_random_rows\n",
    "from upscaler.model import make_upscaler_attention, make_discriminator_simple_512, wasserstein_loss, VGG_LOSS, make_and_compile_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a044c25f35e4bbeb27aee9ba233f82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading files', max=483, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405a73aaca384bf69d36f9046eedce5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Downscaling images', max=1, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec9e236fb7540b3a76b7289df3ed312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading files', max=1664, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad9202b7f4c461e8871ed9b99f6cb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading files', max=1664, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images_fullhd = load_images_from_dir(\n",
    "    '../images/ukiyo-e_fullhd/',\n",
    "    '.jpg',\n",
    "    limit = 4000,\n",
    "    prog_func = tqdm_notebook\n",
    ")\n",
    "images_fullhd = downscale_images(\n",
    "    images_fullhd,\n",
    "    prog_func = tqdm_notebook,\n",
    "    downscale_ratio = 4\n",
    ")\n",
    "images_fullhd = images_fullhd.rename(columns={'image_hr': 'fullhd', 'downscaled' : 'scaled'})\n",
    "\n",
    "images_1gen = load_images_from_dir(\n",
    "    '../images/ukiyo-e_1gen/',\n",
    "    '.jpg',\n",
    "    limit = 4000,\n",
    "    prog_func = tqdm_notebook\n",
    ")\n",
    "images_1gen = images_1gen.rename(columns={'image_hr': 'gen1'}).drop(columns='image_size')\n",
    "\n",
    "images_2gen = load_images_from_dir(\n",
    "    '../images/ukiyo-e_2gen/',\n",
    "    '.jpg',\n",
    "    limit = 4000,\n",
    "    prog_func = tqdm_notebook\n",
    ")\n",
    "images_2gen = images_2gen.rename(columns={'image_hr': 'gen2'}).drop(columns='image_size')\n",
    "\n",
    "images = images_fullhd.join(images_1gen.set_index('filename'), on = 'filename', how = 'inner').join(images_2gen.set_index('filename'), on = 'filename', how = 'inner')\n",
    "\n",
    "images = crop_images_cgc(images, target_shape = (512,512), seed = 42, downscale_ratio = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_compile_gan(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    content_loss,\n",
    "    content_loss_weight,\n",
    "    discriminator_loss,\n",
    "    discriminator_loss_weight,\n",
    "    optimizer=Adam()\n",
    "):\n",
    "    \n",
    "    generator_input_layer = Input(shape=input_shape)\n",
    "    generator_layer = generator(generator_input_layer)\n",
    "    generator_training_model = Model(inputs=generator_input_layer, outputs=generator_layer)\n",
    "    generator_training_model.compile(loss=content_loss, optimizer=optimizer)\n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    discriminator_input_layer = Input(shape=output_shape)\n",
    "    discriminator_layer = discriminator(discriminator_input_layer)\n",
    "    discriminator_training_model = Model(inputs=discriminator_input_layer, outputs=discriminator_layer)\n",
    "    discriminator_training_model.compile(loss=discriminator_loss, optimizer=optimizer)\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    gan_input_layer = Input(shape=input_shape)\n",
    "    gan_generator_layer = generator(gan_input_layer)\n",
    "    gan_discriminator_layer = discriminator(gan_generator_layer)\n",
    "    gan_training_model = Model(inputs=gan_input_layer, outputs=[gan_generator_layer, gan_discriminator_layer])\n",
    "    gan_training_model.compile(\n",
    "        loss=[content_loss, discriminator_loss],\n",
    "        loss_weights=[content_loss_weight, discriminator_loss_weight],\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return generator_training_model, discriminator_training_model, gan_training_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1127 22:14:43.711073 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1127 22:14:43.714115 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1127 22:14:43.771136 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1127 22:14:43.771991 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1127 22:14:43.772803 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1127 22:14:44.883061 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1127 22:14:48.364834 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W1127 22:14:48.377280 140528778274624 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2020: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_vgg_discriminator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8c5fdb19cfc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_upscaler_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'upscaler'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vgg_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'discriminator'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_vgg_discriminator' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape = (128,128,3)\n",
    "output_shape = (512,512,3)\n",
    "\n",
    "generator = make_upscaler_attention(output_shape)\n",
    "generator.name = 'upscaler'\n",
    "discriminator = make_discriminator_simple_512(output_shape)\n",
    "discriminator.name = 'discriminator'\n",
    "\n",
    "vgg_loss = VGG_LOSS(output_shape).loss\n",
    "optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train, disc_train, gan_train = make_and_compile_gan(\n",
    "    generator = generator, discriminator = discriminator,\n",
    "    input_shape = input_shape, output_shape = output_shape,\n",
    "    content_loss = vgg_loss, content_loss_weight = 1,\n",
    "    discriminator_loss = wasserstein_loss, discriminator_loss_weight = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "batch_df = select_random_rows(images, 2)\n",
    "\n",
    "batch_hr = pd.concat([batch_df.cropped_hd, batch_df.cropped_hd, batch_df.cropped_hd], ignore_index=True)\n",
    "batch_lr = pd.concat([batch_df.cropped_gen1, batch_df.cropped_gen2, batch_df.cropped_scaled], ignore_index=True)\n",
    "\n",
    "image_batch_hr  = convert_image_series_to_array(batch_hr)\n",
    "image_batch_lr  = convert_image_series_to_array(batch_lr)\n",
    "image_batch_gen = gen_train.predict(image_batch_lr)\n",
    "\n",
    "disc_real_y = np.ones(2 * 3)\n",
    "disc_fake_y = -disc_real_y\n",
    "\n",
    "image_batch_disc = np.concatenate((image_batch_hr, image_batch_gen), axis=0)\n",
    "image_batch_y = np.concatenate((disc_real_y, disc_fake_y), axis=0)\n",
    "\n",
    "loss_disc = disc_train.train_on_batch(image_batch_disc, image_batch_y)\n",
    "loss_gan, loss_gan_gen, loss_gan_disc = gan_train.train_on_batch(image_batch_lr, [image_batch_hr,disc_real_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-524.1081"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6641275"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.069687985"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_gan_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-733.8154"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_gan_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1331.9447"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df = select_random_rows(images, 2)\n",
    "\n",
    "batch_hr = pd.concat([batch_df.cropped_hd, batch_df.cropped_hd, batch_df.cropped_hd], ignore_index=True)\n",
    "batch_lr = pd.concat([batch_df.cropped_gen1, batch_df.cropped_gen2, batch_df.cropped_scaled], ignore_index=True)\n",
    "\n",
    "image_batch_hr  = convert_image_series_to_array(batch_hr)\n",
    "image_batch_lr  = convert_image_series_to_array(batch_lr)\n",
    "image_batch_gen = gen_train.predict(image_batch_lr)\n",
    "\n",
    "disc_real_y = np.ones(2 * 3)\n",
    "disc_fake_y = -disc_real_y\n",
    "\n",
    "disc_train.train_on_batch(image_batch_gen, disc_fake_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "899.6455"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df = select_random_rows(images, 2)\n",
    "\n",
    "batch_hr = pd.concat([batch_df.cropped_hd, batch_df.cropped_hd, batch_df.cropped_hd], ignore_index=True)\n",
    "batch_lr = pd.concat([batch_df.cropped_gen1, batch_df.cropped_gen2, batch_df.cropped_scaled], ignore_index=True)\n",
    "\n",
    "image_batch_hr  = convert_image_series_to_array(batch_hr)\n",
    "image_batch_lr  = convert_image_series_to_array(batch_lr)\n",
    "image_batch_gen = gen_train.predict(image_batch_lr)\n",
    "\n",
    "disc_real_y = np.ones(2 * 3)\n",
    "disc_fake_y = -disc_real_y\n",
    "\n",
    "disc_train.train_on_batch(image_batch_hr, disc_real_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7420183, 0.075797014, 666.22125]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df = select_random_rows(images, 2)\n",
    "\n",
    "batch_hr = pd.concat([batch_df.cropped_hd, batch_df.cropped_hd, batch_df.cropped_hd], ignore_index=True)\n",
    "batch_lr = pd.concat([batch_df.cropped_gen1, batch_df.cropped_gen2, batch_df.cropped_scaled], ignore_index=True)\n",
    "\n",
    "image_batch_hr  = convert_image_series_to_array(batch_hr)\n",
    "image_batch_lr  = convert_image_series_to_array(batch_lr)\n",
    "image_batch_gen = gen_train.predict(image_batch_lr)\n",
    "\n",
    "disc_real_y = np.ones(2 * 3)\n",
    "disc_fake_y = -disc_real_y\n",
    "\n",
    "image_batch_disc = np.concatenate((image_batch_hr, image_batch_gen), axis=0)\n",
    "image_batch_y = np.concatenate((disc_real_y, disc_fake_y), axis=0)\n",
    "\n",
    "gan_train.train_on_batch(image_batch_lr, [image_batch_hr,disc_real_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0895395e+11],\n",
       "       [-4.7791428e+10],\n",
       "       [-2.0895395e+11],\n",
       "       [-4.7791428e+10],\n",
       "       [-2.0895395e+11],\n",
       "       [-4.7791428e+10],\n",
       "       [ 1.3247290e+08],\n",
       "       [ 1.3499280e+08],\n",
       "       [ 1.3545078e+08],\n",
       "       [ 1.3401422e+08],\n",
       "       [ 1.3526914e+08],\n",
       "       [ 1.3252787e+08]], dtype=float32)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df = select_random_rows(images, 2)\n",
    "\n",
    "batch_hr = pd.concat([batch_df.cropped_hd, batch_df.cropped_hd, batch_df.cropped_hd], ignore_index=True)\n",
    "batch_lr = pd.concat([batch_df.cropped_gen1, batch_df.cropped_gen2, batch_df.cropped_scaled], ignore_index=True)\n",
    "\n",
    "image_batch_hr  = convert_image_series_to_array(batch_hr)\n",
    "image_batch_lr  = convert_image_series_to_array(batch_lr)\n",
    "image_batch_gen = gen_train.predict(image_batch_lr)\n",
    "\n",
    "disc_real_y = np.ones(2 * 3)\n",
    "disc_fake_y = -disc_real_y\n",
    "\n",
    "image_batch_disc = np.concatenate((image_batch_hr, image_batch_gen), axis=0)\n",
    "image_batch_y = np.concatenate((disc_real_y, disc_fake_y), axis=0)\n",
    "\n",
    "disc_train.predict(image_batch_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df = select_random_rows(images, 2)\n",
    "\n",
    "batch_hr = pd.concat([batch_df.cropped_hd, batch_df.cropped_hd, batch_df.cropped_hd], ignore_index=True)\n",
    "batch_lr = pd.concat([batch_df.cropped_gen1, batch_df.cropped_gen2, batch_df.cropped_scaled], ignore_index=True)\n",
    "\n",
    "image_batch_hr  = convert_image_series_to_array(batch_hr)\n",
    "image_batch_lr  = convert_image_series_to_array(batch_lr)\n",
    "image_batch_gen = gen_train.predict(image_batch_lr)\n",
    "\n",
    "disc_real_y = np.ones(2 * 3)\n",
    "disc_fake_y = -disc_real_y\n",
    "\n",
    "image_batch_disc = np.concatenate((image_batch_hr, image_batch_gen), axis=0)\n",
    "image_batch_y = np.concatenate((disc_real_y, disc_fake_y), axis=0)\n",
    "\n",
    "gan_train.predict(image_batch_lr)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
